## 批量归一化（batch normalization）
一种流行且有效的技术，可持续加速深层网络的收敛速度。

## 原因
首先，数据预处理的方式通常会对最终结果产生巨大影响。
第二，对于典型的多层感知机或卷积神经网络。中间层中的变量（例如，多层感知机中的仿射变换输出）可能具有更广的变化范围
第三，更深层的网络很复杂，容易过拟合。

## 原理
批量归一化应用于单个可选层（也可以应用到所有层），其原理如下：在每次训练迭代中，我们首先归一化输入，即通过减去其均值并除以其标准差，其中两者均基于当前小批量处理。 接下来，我们应用比例系数和比例偏移。

## 批量归一化层
全连接层：应用在激活函数之前，通常，我们将批量归一化层置于全连接层中的仿射变换和激活函数之间
卷积层：我们可以在卷积层之后和非线性激活函数之前应用批量归一化，当卷积有多个输出通道时，我们需要对这些通道的“每个”输出执行批量归一化，每个通道都有自己的拉伸（scale）和偏移（shift）参数，这两个参数都是标量。

## 预测
将训练好的模型用于预测时，我们不再需要样本均值中的噪声以及在微批次上估计每个小批次产生的样本方差了。
一种常用的方法是通过移动平均估算整个训练数据集的样本均值和方差，并在预测时使用它们得到确定的输出。