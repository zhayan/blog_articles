## LeNet
用于手写数字识别，MNIST 数据集，28x28
每个卷积块中的基本单元是一个卷积层、一个 sigmoid 激活函数和平均汇聚层。

## AlexNet
深度神经网络
在AlexNet的第一层，卷积窗口的形状是  11×11 。 由于ImageNet中大多数图像的宽和高比MNIST图像的多10倍以上，因此，需要一个更大的卷积窗口来捕获目标。 第二层中的卷积窗口形状被缩减为  5×5 ，然后是  3×3 。 此外，在第一层、第二层和第五层卷积层之后，加入窗口形状为  3×3 、步幅为2的最大汇聚层。 而且，AlexNet的卷积通道数目是LeNet的10倍。
AlexNet将sigmoid激活函数改为更简单的ReLU激活函数。

## VGG
复用网络块
 vgg_block 使用了带有  3×3  卷积核、填充为 1（保持高度和宽度）的卷积层，卷积层数量可定义，和带有  2×2  池化窗口、步幅为 2（每个块后的分辨率减半）的最大汇聚层。

## NiN
NiN使用由一个卷积层和多个  1×1  卷积层组成的块。该块可以在卷积神经网络中使用，以允许更多的每像素非线性。
NiN去除了容易造成过拟合的全连接层，将它们替换为全局平均汇聚层（即在所有位置上进行求和）

## GoogLeNet
基本的卷积块被称为Inception块（Inception block）
Inception块由四条并行路径组成。 前三条路径使用窗口大小为  1×1 、 3×3  和  5×5  的卷积层，从不同空间大小中提取信息。 中间的两条路径在输入上执行  1×1  卷积，以减少通道数，从而降低模型的复杂性。 第四条路径使用  3×3  最大汇聚层，然后使用  1×1  卷积层来改变通道数。 
Inception 块相当于一个有4条路径的子网络。它通过不同窗口形状的卷积层和最大汇聚层来并行抽取信息，并使用  1×1  卷积层减少每像素级别上的通道维数从而降低模型复杂度。

## ResNet
残差网络的核心思想是：每个附加层都应该更容易地包含原始函数作为其元素之一。
残差映射可以更容易地学习同一函数，例如将权重层中的参数近似为零。

## DenseNet
ResNet 和 DenseNet 的关键区别在于，DenseNet 输出是连接（ [,]  表示）而不是如 ResNet 的简单相加。
最后，将这些展开式结合到多层感知机中，再次减少特征的数量。